import os
import re
import subprocess


# Get the absolute workflow path
def get_workflow_path(start_dir="."):
    result = subprocess.run(["find", start_dir, "-name", "Snakefile*"], stdout=subprocess.PIPE, stderr=subprocess.PIPE) # Locate workflow_dir using Snakefile
    stdout_str = result.stdout.decode("utf-8").strip() # Decode stdout into a string
    
    if not stdout_str:
        return None
    workflow_dir = re.sub(r"/workflow/.*", "", stdout_str)
    return os.path.abspath(workflow_dir)


# Parse control file
def parse_control_file ( control_file ):
    with open(control_file) as f:
        first_line = True

        # Skip header line
        for line in f:
            if first_line:
                continue

        columns = line.strip().split("\t")

        if len(columns) != 5:
            raise ValueError("Invalid sample list format. Sample list should be of format: group_name\tsample_name\tfastq_filename1\tfastq_filename2\tindex_filename")

        group = columns[0]
        sample = columns[1]
        filename1 = columns[2]
        filename2 = columns[3]
        index = columns[4]
        
        if sample not in samples_dict:
            samples_dict[sample] = {'group': group, 'filename1': filename1, 'filename2': filename2, 'index': index}
            
            with open ( 'samples.list', "w" ) as file: # Write sample.list file
                print ( f"{sample}\t{group}", file=file )
        else:
            pass

        return ( sample_dict )

    
# ====================================================== VALUE PREPARATION =========================================== #

workflow_dir = get_workflow_path()
configfile: f'{workflow_dir}/config/config.yaml'
scripts = os.path.join(workflow_dir, "workflow/scripts")

# Create wildcards for group, sample names and index files
groups = list(set(sample_info['group'] for sample_info in samples_dict.values()))
samples = [sample_info for sample_info in samples_dict.keys()]
index_files = [sample_info['index'] for sample_info in samples_dict.values()]

# Create comparisons based on sample names
def all_pairs (groups):
    comparisons = {}
    
    for group1, group2 in itertools.combinations(groups, 2):
        comparisons[str(group1+"_"+group2)]=str(group1+"_vs_"+group2)   

    return list(comparisons.values())                   

de_subsets = all_pairs(groups)
log2FC_cutoff = config['log2FC_cutoff']


## ====================================================== ANALYSIS ====================================================== ##

rule all:
    input:
        expand ( "edgeR/02_analyze_DE/{de_subset}.P1e-3_{log2FC_cutoff}.DE.xlsx",
                 de_subset = de_subsets,
                 log2FC_cutoff = log2FC_cutoff )

rule index:
    input: fasta = "{index_files}"
    output: indexed = directory("{index_files}.salmon.index")
    conda: "envs/salmon.yaml"
    shell: """ salmon index -t {input.fasta} -i {output} --keepDuplicates """

    
# Perform alignment based on sample-to-index associations defined in sample_dict
rule selective_alignment:
    input: fastq1=lambda wildcards: f"{wildcards.samples}_1.trimmed.fastq.gz",
           fastq2=lambda wildcards: f"{wildcards.samples}_2.trimmed.fastq.gz",
           indexed=lambda wildcards: directory(f"{samples_dict[wildcards.samples]['index']}.salmon.index")
    output: directory("{samples}.salmon")
    conda: "envs/salmon.yaml"
    threads: config['mapping_threads']
    shell: """ salmon quant -i {input.indexed} \
    	       	      	    -l A -1 {input.fastq1} -2 {input.fastq2} \
			    --threads {threads} \
			    --validateMappings \
			    -o {output} """


# Merge all quant files into  single file
rule quantmerge:
    input: quant_files = ",".join(expand("{sample}.salmon/quant.sf", sample=samples))

    output: counts = "all_samples.counts.tsv",
            tpm = "all_samples.tpm.tsv",
            
    conda: "envs/salmon.yaml"
    message: "Merging counts and TPM values per wildcards.samples"
    shell: """ salmon quantmerge --quants {input.quant_files} \
                                 --column numreads \
                                 --output {output.counts} && \
    
               salmon quantmerge --quants {input.quant_files} \
                                 --column TPM \
                                 --output {output.tpm} """
    
rule PCA:
    input: tpm="all.genes.tpm.tsv",
           samples_list = "samples.list"
    output: pca="PCA.svg"
    conda: "envs/salmon.yaml"
    shell: """ Rscript {scripts}/pca.R {input.tpm} {input.samples_list} """
    
    
rule make_directories:
    input:
        counts_file="all_samples.counts.tsv",
        samples_list= "samples.list",
        pca="PCA.svg"
    output: 'chkp'
    shell: """ mkdir edgeR && \
               cd edgeR && \
               mkdir 01_run_DE_analysis && \
               mkdir 02_analyze_DE && \
               cd ../ && \
               cp results/counts.mod.txt edgeR/01_run_DE_analysis/ && \
               cp {input.samples_list} edgeR/01_run_DE_analysis/ && \
               touch chkp """

rule run_DE_analysis:
    input:
        counts_file="counts.mod.txt",
        samples_list="samples.list",
        chkp='chkp'
    output: 'edgeR/chkp01'
    shell: """ cd edgeR/01_run_DE_analysis && \
               perl {scripts}/run_DE_analysis.pl --matrix ../../../{input.counts_file} --method edgeR --samples_file ../../results/{input.samples_list} && \
               cd ../ && \
               touch chkp01 """

rule analyze_DE:
    input: 'edgeR/chkp01'
    output: 'edgeR/chkp02'
    params: DE_cutoff = config['log2FC_cutoff']
    shell: """ cd edgeR/02_analyze_DE && \
               ln -s ../01_run_DE_analysis/edgeR.*/counts.mod.txt* . && \
               perl {scripts}/analyze_diff_expr.pl --matrix ../../results/counts.mod.txt --samples ../01_run_DE_analysis/samples.list -P 1e-3 -C {params.DE_cutoff} && \
               cd ../ && \
               touch chkp02 && \
               rm ../chkp ../{input} ../{output} """

rule rename:
    input: 'edgeR/02_analyze_DE/counts.mod.txt.{de_subsets}.edgeR.DE_results.P1e-3_{log2FC_cutoff}.DE.subset'
    output: 'edgeR/02_analyze_DE/{de_subsets}.P1e-3_C{log2FC_cutoff}.DE.subset'
    shell: " perl {scripts}/rename.pl {input} "
    
    
rule reverse_sort:
     input: 'edgeR/02_analyze_DE/{de_subsets}.P1e-3_{log2FC_cutoff}.DE.tsv'
     output: sorted = 'edgeR/02_analyze_DE/{de_subsets}.P1e-3_{log2FC_cutoff}.DE.tsv'            
     shell: """ perl {scripts}/reverse_sort.pl {input} > {output.sorted} """

     
rule tsv2xlsx:
     input: 'edgeR/02_analyze_DE/{de_subsets}.P1e-3_{log2FC_cutoff}.DE.tsv'
     output: 'edgeR/02_analyze_DE/{de_subsets}.P1e-3_{log2FC_cutoff}.DE.xlsx'
     shell: """ python3 {scripts}/tsv2xlsx.py {input} """
